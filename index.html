<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes">
  <meta property="og:title" content="Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes"/>
  <meta property="og:description" content="We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations."/>
  <meta property="og:url" content="https://weakcubercnn.compute.dtu.dk/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/sun.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes">
  <meta name="twitter:description" content="We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/sun.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Weak Supervision, 3D Object Detection, Monocular Object Detection, weak cube rcnn, cube rcnn, 3D object detection, weakly supervised 3D object detection, 3D detection, monocular 3D object detection, computer vision, robotics, virtual reality, machine learning, deep learning, artificial intelligence, AI, computer vision research, image analysis, object recognition, depth estimation, orientation estimation, pseudo-ground truth, loss functions, foundation models, large-scale datasets">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Weak Cube R-CNN</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/andreaslh/" target="_blank">Andreas Lau Hansen</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/lukas-wanzeck/" target="_blank">Lukas Wanzeck</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://orbit.dtu.dk/en/persons/dimitrios-papadopoulos" target="_blank">Dim P. Papadopoulos</a><sup>1,2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Technical University of Denmark<br>

                      23rd Scandinavian Conference on Image Analysis, SCIA 2025
                      </span>
                    <span class="eql-cntrb"><small><br><sup>1</sup>Technical University of Denmark</small>,</span>
                    <span class="eql-cntrb"><small><sup>2</sup>Pioneer Centre for AI</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2504.13297.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AndreasLH/Weak-Cube-R-CNN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- Huggingface link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/AndreasLH/Weak-Cube-R-CNN" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      ðŸ¤—
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/Fig1_v3.png" alt="Figure 1" />
          Weak Cube R-CNN. In contrast to standard 3D object detectors that require 3D ground truths, our proposed method is trained using only 2D bounding boxes but can predict 3D cubes at test time. Weak Cube R-CNN significantly reduces the annotation time since 3D ground-truths require 11Ã— more time than annotating 2D boxes. More importantly, it does not require access to LiDAR or multi-camera setups.
        </div>
      </div>
    </div>
  </div>
</section></section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotate. This work focuses on weakly-supervised 3D detection to reduce data needs using a monocular method that leverages a single-camera system over expensive LiDAR sensors or multi-camera setups. We propose a general model Weak Cube R-CNN, which can predict objects in 3D at inference time, requiring only 2D box annotations for training by exploiting the relationship between 2D projections of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D models to estimate depth and orientation information on a training set. We use these estimated values as pseudo-ground truths during training. We design loss functions that avoid 3D labels by incorporating information from the external models into the loss. In this way, we aim to implicitly transfer knowledge from these large foundation 2D models without having access to 3D bounding box annotations. Experimental results on the SUN RGB-D dataset show increased performance in accuracy compared to an annotation time equalized <a href="https://garrickbrazil.com/omni3d/"><u>Cube R-CNN baseline</u></a>. While not precise for centimetre-level measurements, this method provides a strong foundation for further research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Model overview</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/Fig2_v3.png" alt="Figure 1" />
          Overview of Weak Cube R-CNN . The model extracts features from an image and predicts objects in 2D and their cubes in 3D. We split the cube into each of its attributes and optimise each attribute with regards to a pseudo ground truth information. During training, instead of the simple 3D ground truth provided in the fully supervised setting, we must use many different sources of information provided by frozen models to emulate the same ground truth annotation.
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Pipeline Overview</h2>

      Using a combination of the GroundingDINO and Segment Anything methods (middle image), effectively filters the ground. Now you can run a simple plane-estimation RANSAC algorithm on the filtered point cloud, this turns out to be quite robust.
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/pipeline.png" alt="Figure 1" />
          Ground estimation pipeline showing the point cloud obtained through the depth map. The 2nd step selects the region in the depth map corresponding to the ground in the color image. The depth map is interpreted as a point cloud where planeRANSAC obtains a normal vector to the ground.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Prediction Examples</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/sun.png" alt="sun rgbd results"/>
        <h2 class="subtitle has-text-centered">
          Qualitative examples of Weak Cube R-CNN predictions on <b>SUN-RGBD</b> test set. Images are selected to showcase behaviour in various scenarios. Only the last row is shown with ground truths in red to avoid clutter. In the last row ground truths are shown in red with predictions in green. Each image is shown side-by-side with its corresponding top-view image, where each square is 1x1 m.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/kitti.png" alt="kitti results"/>
        <h2 class="subtitle has-text-centered">
          Qualitative examples of Weak Cube R-CNN predictions on <b>KITTI</b> test set. KITTI predictions are shown in green and ground truth in red. Each image is shown with its corresponding top-view image, where each square is 1x1 m.
        </h2>
      </div>
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Just for fun</h2>
      What happens when you try to run the model on completely out of domain data?

      You get some very fun results. Just look at how it tries to estimate the dimensions to be several meters, while the hand is clearly showing the scale as few centimeters!
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/ood.png" alt="Figure ood" />
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <iframe  src="static/pdfs/weakcubercnn.pdf" width="100%" height="800">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{hansen2025weakcubercnnweakly,
        title={Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes}, 
        author={Andreas Lau Hansen and Lukas Wanzeck and Dim P. Papadopoulos},
        year={2025},
        eprint={2504.13297},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2504.13297}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
